<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    }
  };
</script>



  <meta name="description" content="论文： Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift     Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models">
<meta name="keywords" content="ObjectDetection">
<meta property="og:type" content="article">
<meta property="og:title" content="batchnorm-总结">
<meta property="og:url" content="http://yoursite.com/2018/09/04/batchnorm-总结/index.html">
<meta property="og:site_name" content="友人帐">
<meta property="og:description" content="论文： Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift     Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/images/bn/1.png">
<meta property="og:image" content="http://yoursite.com/images/bn/2.png">
<meta property="og:image" content="http://yoursite.com/images/bn/3.png">
<meta property="og:image" content="http://yoursite.com/images/bn/4.png">
<meta property="og:image" content="http://yoursite.com/images/bn/6.png">
<meta property="og:image" content="http://yoursite.com/images/bn/5.png">
<meta property="og:updated_time" content="2019-07-14T07:17:36.259Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="batchnorm-总结">
<meta name="twitter:description" content="论文： Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift     Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models">
<meta name="twitter:image" content="http://yoursite.com/images/bn/1.png">





  
  
  <link rel="canonical" href="http://yoursite.com/2018/09/04/batchnorm-总结/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>batchnorm-总结 | 友人帐</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">友人帐</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/04/batchnorm-总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="lanfang">
      <meta itemprop="description" content="个人小站，论文笔记加杂记~~~欢迎打赏~">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="友人帐">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">batchnorm-总结

              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-09-04 10:19:31" itemprop="dateCreated datePublished" datetime="2018-09-04T10:19:31+08:00">2018-09-04</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-07-14 15:17:36" itemprop="dateModified" datetime="2019-07-14T15:17:36+08:00">2019-07-14</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/notes/" itemprop="url" rel="index"><span itemprop="name">notes</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/notes/object-detection/" itemprop="url" rel="index"><span itemprop="name">object detection</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>论文：</p>
<p><a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift </a>   </p>
<p><a href="https://arxiv.org/abs/1702.03275" target="_blank" rel="noopener">Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models </a></p>
<a id="more"></a>

<h1 id="一、batchnormalization简介"><a href="#一、batchnormalization简介" class="headerlink" title="一、batchnormalization简介"></a>一、batchnormalization简介</h1><h2 id="1-源起"><a href="#1-源起" class="headerlink" title="1.源起"></a>1.源起</h2><h3 id="Internal-Covariate-Shift"><a href="#Internal-Covariate-Shift" class="headerlink" title="Internal Covariate Shift"></a>Internal Covariate Shift</h3><p><img src="/images/bn/1.png" alt></p>
<p>简而言之就是：在网络训练过程中，随着网络层数加深，网络的输入会很大，跑到激活函数的饱和区，使得梯度消失。<strong>我们定义：在训练时因为网络参数的改变而导致的网络激活的输出的分布的改变，成为内部协变量漂移。</strong></p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol>
<li>学习率要很小</li>
<li>初始化要谨慎</li>
<li>容易非线性饱和</li>
</ol>
<h2 id="2-bn做法"><a href="#2-bn做法" class="headerlink" title="2.bn做法"></a>2.bn做法</h2><h3 id="normal-operation"><a href="#normal-operation" class="headerlink" title="normal operation"></a>normal operation</h3><p><img src="/images/bn/2.png" alt></p>
<blockquote>
<p>For instance, normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity. To address this, we make sure that the transformation inserted in the network can represent the identity transform. To accomplish this, we introduce, for each activation x(k), a pair of parameters<br>γ(k), β(k), which scale and shift the normalized value.</p>
</blockquote>
<p>相对于普通标准化，这里引入两个变量：$\gamma,\beta$，为了让输出不仅仅是在非线性激活的过原点附近的线性区(以sigmod激活为例).为了适当保留其非线性，使用这两个参数进行分布的调整，至于调整多少，由训练决定。</p>
<h3 id="bn-in-cnn"><a href="#bn-in-cnn" class="headerlink" title="bn in cnn"></a>bn in cnn</h3><p><img src="/images/bn/3.png" alt></p>
<p><a href="https://stackoverflow.com/questions/38553927/batch-normalization-in-convolutional-neural-network?newreg=8055ada6494d49c4b4b25769e576c264" target="_blank" rel="noopener">解释：</a></p>
<p>cnn层的输出是4维tensor， <code>[B, H, W, C]</code>,  <code>B</code> 是batch size, <code>(H, W)</code> 是 <em>feature map<em>大小, <code>C</code> 通道数. 其中索引 <code>(x, y)</code> ， <code>0 &lt;= x &lt; H</code> 和 <code>0 &lt;= y &lt; W</code> 是空间位置</em>spatial location</em>.</p>
<h4 id="usual-bn"><a href="#usual-bn" class="headerlink" title="usual bn"></a>usual bn</h4><p>是使用<code>B</code>元素计算出<code>H*W*C</code> 个均值和 <code>H*W*C</code>标准差 .即不同层的不同位置的元素有自己的均值和标准差(<code>B</code>个元素计算出的).</p>
<h4 id="cnn-bn"><a href="#cnn-bn" class="headerlink" title="cnn bn"></a>cnn bn</h4><p>cnn的参数对于整张input图像共享。所以我们将每张特征图看作是一个激活，也就是使用<code>B*H*W</code>个元素计算<code>C</code>个均值和方差。</p>
<h2 id="3-bn反传"><a href="#3-bn反传" class="headerlink" title="3.bn反传"></a>3.bn反传</h2><p><img src="/images/bn/4.png" alt></p>
<p><a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html" target="_blank" rel="noopener">还有教你怎么写bn层反传的blog</a></p>
<h2 id="4-bn训练和推断过程"><a href="#4-bn训练和推断过程" class="headerlink" title="4.bn训练和推断过程"></a>4.bn训练和推断过程</h2><p><img src="/images/bn/6.png" alt></p>
<h1 id="二、some-questions"><a href="#二、some-questions" class="headerlink" title="二、some questions"></a>二、some questions</h1><h2 id="1-Training-vs-Testing"><a href="#1-Training-vs-Testing" class="headerlink" title="1.Training vs Testing"></a>1.Training vs Testing</h2><ul>
<li>训练过程中，需要计算出mini-batch的均值和标准差来规范化该batch，<strong>同时使用moving average的方式计算population mean和variance，以供测试时使用</strong>。</li>
<li>在inference时，使用预先计算出来的参数进行mini-batch的统计。</li>
</ul>
<p>moving average的计算入下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">running_mean = momentum*running_mean+(<span class="number">1</span>-momentum)*sample_mean</span><br><span class="line">running_variance = momentum*running_variance+(<span class="number">1</span>-momentum)*running_variance</span><br></pre></td></tr></table></figure>

<h2 id="2-关于偏置"><a href="#2-关于偏置" class="headerlink" title="2.关于偏置"></a>2.关于偏置</h2><p>使用batch-normalization之后，不需要偏置b，因为任何偏置最后都会因为正规化输出而被被cancel掉,也就是偏置是无法学习的。</p>
<h2 id="3-Batch-Normalization-as-Regularization"><a href="#3-Batch-Normalization-as-Regularization" class="headerlink" title="3.Batch Normalization as Regularization"></a>3.Batch Normalization as Regularization</h2><p><strong>dropout</strong>是网络训练正则化的一项重要手段：</p>
<ul>
<li>它引入了噪声，因此使得网络在训练过程中能够学习处理这些噪声，因而能够泛化的更强。</li>
<li>另一个角度是：类似于bagging的思想，每次随机取0.5的数据，且数据不尽相同，可以大大提高模型的泛化能力。</li>
</ul>
<p><strong>batch-norm</strong>恰巧在训练时候也有引入噪声的功效：</p>
<p><img src="/images/bn/5.png" alt></p>
<p>解释是：</p>
<p>将dropout看成加入了一个噪声向量[0,0,0,1,1，……0]，早就有论文论证过在梯度中加入噪声可以提高模型的accuracy。<a href="https://arxiv.org/abs/1511.06807" target="_blank" rel="noopener"><em>Adding Gradient Noise Improves Learning for Very Deep Networks</em></a></p>
<p>batch-norm和dropout类似，他在训练过程中给每个隐层单元乘上随机值。<strong>这个随机值便是mini-batch中的标准差，</strong>因为每个minibatch的数据都是随机挑选的，因此标准差也随机波动。<strong>同时也会减去一个随机的均值。</strong></p>
<p><a href="https://medium.com/@SeoJaeDuk/deeper-understanding-of-batch-normalization-with-interactive-code-in-tensorflow-manual-back-1d50d6903d35" target="_blank" rel="noopener">deep-understanding-of-batchnorm</a></p>
<h2 id="4-Benefits"><a href="#4-Benefits" class="headerlink" title="4.Benefits"></a>4.Benefits</h2><ol>
<li>网络训练的更快：虽然训练过程中，额外的正则化参数和超参数(rescale,recenter)计算，但是网络收敛的更快。所以网络能够训练的更快。</li>
<li>可以使用更大的学习率：梯度下降需要学习率很小保证网络能够收敛。当网络变得更深的时候，梯度在后向传播的时候就会变得很小，需要迭代次数也更多。使用bn之后，可以提高学习率，加快训练。</li>
<li>权重初始化更简单：bn能够减少初始权重的敏感度。</li>
<li>可以使用更多的激活。</li>
<li>创建深层网络的方式更加简单。</li>
<li>一定程度正则化。</li>
</ol>
<h2 id="5-why-use-mini-average-rather-than-moving-average-in-training"><a href="#5-why-use-mini-average-rather-than-moving-average-in-training" class="headerlink" title="5.why use mini-average rather than moving average in training"></a>5.why use mini-average rather than moving average in training</h2><blockquote>
<p>It is natural to ask whether we could simply use the <strong>moving averages µ, σ</strong> to perform the normalization during training, since this would <strong>remove the dependence of the normalized activations on the other example in the minibatch</strong>. <strong>This, however, has been observed to lead to the model blowing up. As argued in [6], such use of moving averages would cause the gradient optimization and the normalization to counteract each other.</strong> For example, the gradient step may increase a bias or scale the convolutional weights, in spite of the fact that the normalization would cancel the effect of these changes on the loss. This would result in unbounded growth of model parameters without actually improving the loss. It is thus crucial to use the minibatch moments, and to backpropagate through them. </p>
</blockquote>
<p>出自：Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models </p>
<p>解释一下：在原论文中有一段话，说了如果激活函数的bn的计算不依赖于minibatch中的其他元素，那么会导致模型爆炸。</p>
<blockquote>
<p>However, if these modifications are interspersed with the optimization steps, then the gradient descent step may attempt to update the parameters in a way that requires the normalization to be updated, which reduces the effect of the gradient step. For example, consider a layer with the input u that adds the learned bias b, and normalizes the result by subtracting the mean of the activation computed over the training data: xb = x - E[x] where x = u + b, X = {x1…N } is the set of values of x over the training set, and E[x] = N1 PN i=1 xi. If a gradient descent step ignores the dependence of E[x] on b, then it will update b ← b + ∆b, where ∆b ∝ -∂ℓ/∂xb. Then u + (b + ∆b) - E[u + (b + ∆b)] = u + b - E[u + b].<br><strong>Thus, the combination of the update to b and subsequent change in normalization led to no change in the output of the layer nor, consequently, the loss. As the training continues, b will grow indefinitely while the loss remains fixed.</strong> This problem can get worse if the normalization not only centers but also scales the activations. We have observed this empirically in initial experiments, where the model blows up when the normalization parameters are computed outside the gradient descent step. </p>
</blockquote>
<p>原本第m个mini-batch的均值计算如下：<br>$$<br>\mu_m = \mu_{pre}+\frac{1}{N}\sum_i^N(x_i)=\mu_{pre}+ \mu_m^*<br>$$</p>
<p>$$<br>\hat{x} = x-\mu_m<br>$$</p>
<p>比如说，我们更新了一个参数<br>$$<br>x’=x+\Delta x<br>$$</p>
<p>$$<br>\mu_m’=\mu_{pre}+(\mu_m^<em>)’\=\mu_{pre}+(N\mu_m^</em>+\sum_i^N(\Delta x_i))/N=\mu_{pre}+\mu_m^*+\sum_i^N(\Delta x_i)=\mu_m+\Delta \mu<br>$$</p>
<p>而采用普通的moving average（非batch renormalization中的moving average计算方式）之后的bn之后的:</p>
<p>$$\hat{x}’ =x+\Delta x-(\mu_m’)$$</p>
<p>其中$\mu_m$是前面所以mini-batch的平均值，和本次$x$的更新无关。</p>
<p>所以<br>$$<br>\hat{x}’=x+\Delta x-(\mu_m+\Delta \mu)\=<br>$$</p>
<p>emmmmmmmm,推不下去了，反正意思就是说，使用滑动平均的话，其均值就会不依赖于本次mini-batch中的其他数据(我觉得意思就是，这个均值相当于是个定值？？？)。但是！这会使得normalization和SGD的过程混杂在一起，抵消了SGD的梯度更新的作用！比如，上面引用中说的，梯度更新使得某个参数：<strong>比如偏置更新了，或者缩放了权重，但是normalization之后其输出根本没变化，即更新参数之后对激活的输出没有影响，也即loss不会改变。最后模型参数就会无限增长，导致模型爆炸。</strong></p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol>
<li><a href="https://www.youtube.com/watch?v=em6dfRxYkYU" target="_blank" rel="noopener">youtube:Andrew Wu:C2W3L04</a></li>
<li><a href="https://medium.com/@SeoJaeDuk/deeper-understanding-of-batch-normalization-with-interactive-code-in-tensorflow-manual-back-1d50d6903d35" target="_blank" rel="noopener">deep-understanding-of-batchnorm</a></li>
<li><a href="https://www.cnblogs.com/ranjiewen/articles/7748232.html" target="_blank" rel="noopener">某中文博客</a></li>
<li><a href="https://stats.stackexchange.com/questions/283641/why-does-batch-normalization-use-mini-batch-statistics-instead-of-the-moving-ave" target="_blank" rel="noopener">wny doer bn use minibatch average instead of moving average </a></li>
</ol>

      
    </div>

    

    
      
    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/ObjectDetection/" rel="tag"># ObjectDetection</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/27/MegDet/" rel="next" title="MegDet&&IOU-Net">
                <i class="fa fa-chevron-left"></i> MegDet&&IOU-Net
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/10/13/DetNet/" rel="prev" title="DetNet">
                DetNet <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">lanfang</p>
              <div class="site-description motion-element" itemprop="description">个人小站，论文笔记加杂记~~~欢迎打赏~</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">34</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">19</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">10</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/galaxy-fangfang" title="GitHub &rarr; https://github.com/galaxy-fangfang" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://www.kaggle.com/konglanfang" title="Kaggle &rarr; https://www.kaggle.com/konglanfang" rel="noopener" target="_blank"><i class="fa fa-fw fa-kaggle"></i>Kaggle</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://weibo.com/u/2977394557?refer_flag=1001030101_" title="微博 &rarr; https://weibo.com/u/2977394557?refer_flag=1001030101_" rel="noopener" target="_blank"><i class="fa fa-fw fa-globe"></i>微博</a>
                </span>
              
            </div>
          

          

          
          

          
        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#一、batchnormalization简介"><span class="nav-number">1.</span> <span class="nav-text">一、batchnormalization简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-源起"><span class="nav-number">1.1.</span> <span class="nav-text">1.源起</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Internal-Covariate-Shift"><span class="nav-number">1.1.1.</span> <span class="nav-text">Internal Covariate Shift</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#问题"><span class="nav-number">1.1.2.</span> <span class="nav-text">问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-bn做法"><span class="nav-number">1.2.</span> <span class="nav-text">2.bn做法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#normal-operation"><span class="nav-number">1.2.1.</span> <span class="nav-text">normal operation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bn-in-cnn"><span class="nav-number">1.2.2.</span> <span class="nav-text">bn in cnn</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#usual-bn"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">usual bn</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cnn-bn"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">cnn bn</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-bn反传"><span class="nav-number">1.3.</span> <span class="nav-text">3.bn反传</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-bn训练和推断过程"><span class="nav-number">1.4.</span> <span class="nav-text">4.bn训练和推断过程</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#二、some-questions"><span class="nav-number">2.</span> <span class="nav-text">二、some questions</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Training-vs-Testing"><span class="nav-number">2.1.</span> <span class="nav-text">1.Training vs Testing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-关于偏置"><span class="nav-number">2.2.</span> <span class="nav-text">2.关于偏置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Batch-Normalization-as-Regularization"><span class="nav-number">2.3.</span> <span class="nav-text">3.Batch Normalization as Regularization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Benefits"><span class="nav-number">2.4.</span> <span class="nav-text">4.Benefits</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-why-use-mini-average-rather-than-moving-average-in-training"><span class="nav-number">2.5.</span> <span class="nav-text">5.why use mini-average rather than moving average in training</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">3.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lanfang</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.2.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>










  
  













  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>




  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/affix.js?v=7.2.0"></script>

  <script src="/js/schemes/pisces.js?v=7.2.0"></script>



  
  <script src="/js/scrollspy.js?v=7.2.0"></script>
<script src="/js/post-details.js?v=7.2.0"></script>



  <script src="/js/next-boot.js?v=7.2.0"></script>

  

  

  

  

  


  


  




  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  

  

  

  

  

  

  

  

  

  


  

</body>
</html>
